# MS-APPT Configuration File
# Multi-Scale Affinity Protein-Protein Transformer

# Data Configuration
data:
  train_path: "train.csv"
  test_path: "test.csv"
  cache_dir: "embeddings_cache"
  
  # Preprocessing parameters
  preprocessing:
    min_length: 0
    max_length: 50000
    valid_amino_acids: null  # Don't filter - ESM-2 handles non-standard AAs
    remove_duplicates: false
    
  # Train/validation split
  validation_split: 0.02
  stratify_by: "pkd"
  random_seed: 42

# Model Architecture
model:
  # ESM-2 encoder configuration
  encoder:
    model_name: "facebook/esm2_t36_3B_UR50D"
    embedding_dim: 2560  # ESM-2 650M has 1280 hidden dim
    freeze_epochs: 5
    fine_tune_lr_factor: 0.1
  
  # Projection layer
  projection:
    output_dim: 384  # Project from 1280 to 256 for efficiency
    activation: "relu"
    dropout: 0.1
    
  # Transformer encoder configuration
  transformer:
    num_layers: 4  # Reduced from 3
    num_heads: 8
    dim_feedforward_multiplier: 4  # dim_feedforward = hidden_dim * multiplier
    dropout: 0.3
    activation: "gelu"
    norm_first: true  # Pre-norm for stability
    
  # Multi-scale convolution configuration
  multi_scale_conv:
    kernel_sizes: [3, 5, 7]  # Local motifs, secondary structures
    num_filters: 128  # Filters per kernel size
    dropout: 0.1
    
  # Cross-attention configuration
  cross_attention:
    num_heads: 8  # 256 / 8 = 32 dim per head
    hidden_dim: 384  # Match projection dim
    dropout: 0.35
    
  # Interface gating configuration
  interface_gating:
    hidden_dim: 192  # Hidden dimension for gating network
    dropout: 0.1
    
  # Adaptive pooling configuration  
  adaptive_pooling:
    top_k_ratio: 0.1  # Consider top 10% positions as hotspots
    
  # Simple pooling (kept for compatibility)
  pooling:
    methods: ["mean"]  # Just mean pooling
    
  # Affinity head configuration (simplified like APPT)
  mlp:
    hidden_dims: [384, 192]  # Two hidden layers only
    activation: "gelu"
    dropout: 0.3  # Moderate dropout
    use_layer_norm: true
    use_residual: false  # No residual connections
    
# Training Configuration
training:
  # Optimizer settings
  optimizer:
    type: "AdamW"
    learning_rate: 5e-5
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 2e-4
    
  # Learning rate scheduler
  scheduler:
    type: "CosineAnnealingLR"
    eta_min: 1e-6
    
  # Training parameters
  batch_size: 8  # Reduced due to larger model
  gradient_accumulation_steps: 4  # Effective batch size = 32
  max_epochs: 200
  early_stopping:
    patience: 30
    metric: "validation_rmse"
    mode: "min"
    
  # Mixed precision and optimization
  mixed_precision: true
  gradient_clipping:
    max_norm: 10.0
    
  # Checkpointing
  checkpoint_dir: "checkpoints"
  save_best_only: true
  save_last_k: 5
  
# Loss and Metrics
loss:
  type: "mse"
  reduction: "mean"
  
metrics:
  regression:
    - "mse"
    - "rmse"
    - "mae"
    - "r2"
    - "pearson"
    - "spearman"
  domain_specific:
    - "fraction_within_1"
    - "fraction_within_2"
    - "relative_error"
    
# Hardware and Performance
hardware:
  device: "cuda"
  num_workers: 8
  pin_memory: true
  
# Logging Configuration
logging:
  log_dir: "logs"
  log_level: "INFO"
  use_wandb: false
  wandb_project: "ms-appt"
  use_tensorboard: true
  log_every_n_steps: 10
  
# Reproducibility
reproducibility:
  seed: 42
  deterministic: false
  benchmark: true
  
# Inference Configuration
inference:
  batch_size: 128
  use_cache: true
  output_dir: "predictions"
  save_attention_weights: false
  
# Visualization
visualization:
  output_dir: "figures"
  formats: ["png", "pdf"]
  dpi: 300
  plot_types:
    - "scatter"
    - "error_distribution"
    - "residuals"
    - "stratified_performance"