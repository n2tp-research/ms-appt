# MS-APPT Configuration File
# Multi-Scale Affinity Protein-Protein Transformer

# Data Configuration
data:
  train_path: "train.csv"
  test_path: "test.csv"
  cache_dir: "embeddings_cache"
  
  # Preprocessing parameters
  preprocessing:
    min_length: 30
    max_length: 3000
    valid_amino_acids: "ACDEFGHIKLMNPQRSTVWY"
    remove_duplicates: true
    
  # Train/validation split
  validation_split: 0.1
  stratify_by: "pkd"
  random_seed: 42

# Model Architecture
model:
  # ESM-2 encoder configuration
  encoder:
    model_name: "facebook/esm2_t33_650M_UR50D"
    embedding_dim: 1280  # ESM-2 650M has 1280 hidden dim
    freeze_epochs: 5
    fine_tune_lr_factor: 0.1
  
  # Projection layer
  projection:
    output_dim: 384  # Project from 1280 to 384
    activation: "relu"
    dropout: 0.1
    
  # Multi-scale convolution
  conv_layers:
    kernel_sizes: [3, 5, 7]
    num_filters: 64  # Reduced from 96
    activation: "relu"
    dropout: 0.1
    
  # Self-attention configuration
  self_attention:
    num_heads: 4
    hidden_dim: 384
    dropout: 0.3  # Increased dropout
    num_layers: 4  # Add attention layers parameter
    
  # Cross-attention configuration
  cross_attention:
    num_heads: 4
    hidden_dim: 384
    dropout: 0.3  # Increased dropout
    num_layers: 4  # Add attention layers parameter
    
  # Pooling strategies
  pooling:
    methods: ["mean", "max", "attention"]
    attention_hidden_dim: 256
    
  # MLP head configuration
  mlp:
    hidden_dims: [384, 160, 64]  # Scaled down significantly
    activation: "gelu"
    dropout: 0.3  # Increased dropout
    use_layer_norm: true
    use_residual: false  # Disable residual for smaller network
    
# Training Configuration
training:
  # Optimizer settings
  optimizer:
    type: "AdamW"
    learning_rate: 5e-5
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 2e-4
    
  # Learning rate scheduler
  scheduler:
    type: "CosineAnnealingLR"
    eta_min: 1e-6
    
  # Training parameters
  batch_size: 32  # Reduced due to larger model
  gradient_accumulation_steps: 1  # Effective batch size = 32
  max_epochs: 50
  early_stopping:
    patience: 10
    metric: "validation_rmse"
    mode: "min"
    
  # Mixed precision and optimization
  mixed_precision: true
  gradient_clipping:
    max_norm: 1.0
    
  # Checkpointing
  checkpoint_dir: "checkpoints"
  save_best_only: true
  save_last_k: 5
  
# Loss and Metrics
loss:
  type: "mse"
  reduction: "mean"
  
metrics:
  regression:
    - "mse"
    - "rmse"
    - "mae"
    - "r2"
    - "pearson"
    - "spearman"
  domain_specific:
    - "fraction_within_1"
    - "fraction_within_2"
    - "relative_error"
    
# Hardware and Performance
hardware:
  device: "cuda"
  num_workers: 8
  pin_memory: true
  
# Logging Configuration
logging:
  log_dir: "logs"
  log_level: "INFO"
  use_wandb: false
  wandb_project: "ms-appt"
  use_tensorboard: true
  log_every_n_steps: 10
  
# Reproducibility
reproducibility:
  seed: 42
  deterministic: false
  benchmark: true
  
# Inference Configuration
inference:
  batch_size: 64
  use_cache: true
  output_dir: "predictions"
  save_attention_weights: false
  
# Visualization
visualization:
  output_dir: "figures"
  formats: ["png", "pdf"]
  dpi: 300
  plot_types:
    - "scatter"
    - "error_distribution"
    - "residuals"
    - "stratified_performance"